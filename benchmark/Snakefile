import linopy
import pyomo

NRANGE = [20, 50, 100, 180, 400, 600, 800, 1200]
#NRANGE = [10, 20]
SOLVERS = ["gurobi"]
APIS = ['linopy', 'jump', 'pyomo', 'pulp', 'gurobipy', 'solver']


rule all:
    input: expand("benchmarks/{solver}/benchmark-{kind}.pdf", solver=SOLVERS, kind=['absolute', 'overhead'])

# LINOPY
rule benchmark_time_linopy:
    params:
        nrange = NRANGE,
    output:
        "benchmarks/{solver}/linopy/time.csv"
    script: "scripts/benchmark_linopy.py"

rule benchmark_memory_single_linopy:
    params:
        nrange = NRANGE,
    benchmark:
        "benchmarks/{solver}/linopy/memory-{N}.txt"
    script: "scripts/run-linopy.py"

rule benchmark_linopy:
    params:
        nrange = NRANGE,
        api = "linopy",
    input:
        memory = lambda w: expand("benchmarks/{solver}/linopy/memory-{N}.txt", N=NRANGE, solver=w.solver),
        time = "benchmarks/{solver}/linopy/time.csv",
    output:
        benchmark = "benchmarks/{solver}/linopy.csv"
    script: "scripts/concat-benchmarks.py"


# PULP
rule benchmark_time_pulp:
    params:
        nrange = NRANGE,
    output:
        "benchmarks/{solver}/pulp/time.csv"
    script: "scripts/benchmark_pulp.py"

rule benchmark_memory_single_pulp:
    params:
        nrange = NRANGE,
    benchmark:
        "benchmarks/{solver}/pulp/memory-{N}.txt"
    script: "scripts/run-pulp.py"

rule benchmark_pulp:
    params:
        nrange = NRANGE,
        api = "pulp",
    input:
        memory = lambda w: expand("benchmarks/{solver}/pulp/memory-{N}.txt", N=NRANGE, solver=w.solver),
        time = "benchmarks/{solver}/pulp/time.csv",
    output:
        benchmark = "benchmarks/{solver}/pulp.csv"
    script: "scripts/concat-benchmarks.py"

# PYOMO
rule benchmark_time_pyomo:
    params:
        nrange = NRANGE,
    output:
        "benchmarks/{solver}/pyomo/time.csv"
    script: "scripts/benchmark_pyomo.py"

rule benchmark_memory_single_pyomo:
    params:
        nrange = NRANGE,
    benchmark:
        "benchmarks/{solver}/pyomo/memory-{N}.txt"
    script: "scripts/run-pyomo.py"

rule benchmark_pyomo:
    params:
        nrange = NRANGE,
        api = "pyomo",
    input:
        memory = lambda w: expand("benchmarks/{solver}/pyomo/memory-{N}.txt", N=NRANGE, solver=w.solver),
        time = "benchmarks/{solver}/pyomo/time.csv",
    output:
        benchmark = "benchmarks/{solver}/pyomo.csv"
    script: "scripts/concat-benchmarks.py"


# GUROBIPY
rule benchmark_time_gurobipy:
    params:
        nrange = NRANGE,
    output:
        "benchmarks/{solver}/gurobipy/time.csv"
    script: "scripts/benchmark_gurobipy.py"

rule benchmark_memory_single_gurobipy:
    params:
        nrange = NRANGE,
    benchmark:
        "benchmarks/{solver}/gurobipy/memory-{N}.txt"
    script: "scripts/run-gurobipy.py"

rule benchmark_gurobipy:
    params:
        nrange = NRANGE,
        api = "gurobipy",
    input:
        memory = lambda w: expand("benchmarks/{solver}/gurobipy/memory-{N}.txt", N=NRANGE, solver=w.solver),
        time = "benchmarks/{solver}/gurobipy/time.csv",
    output:
        benchmark = "benchmarks/{solver}/gurobipy.csv"
    script: "scripts/concat-benchmarks.py"

# ORTOOLS
rule benchmark_time_ortools:
    params:
        nrange = NRANGE,
    output:
        "benchmarks/{solver}/ortools/time.csv"
    script: "scripts/benchmark_ortools.py"

rule benchmark_memory_single_ortools:
    params:
        nrange = NRANGE,
    benchmark:
        "benchmarks/{solver}/ortools/memory-{N}.txt"
    script: "scripts/run-ortools.py"

rule benchmark_ortools:
    params:
        nrange = NRANGE,
        api = "ortools",
    input:
        memory = lambda w: expand("benchmarks/{solver}/ortools/memory-{N}.txt", N=NRANGE, solver=w.solver),
        time = "benchmarks/{solver}/ortools/time.csv",
    output:
        benchmark = "benchmarks/{solver}/ortools.csv"
    script: "scripts/concat-benchmarks.py"



rule benchmark_jump:
    "For time & memory benchmarks, use one process for all runs (skip import and jit compilation times)"
    params:
        nrange = NRANGE,
    output:
        "benchmarks/{solver}/jump.csv"
    script: "scripts/benchmark_jump.jl"


# For benchmarking solver processes only, we have to start from the lp files
rule write_lp:
    output:
        expand("benchmarks/lp_files/{N}.lp", N=NRANGE)
    script: "scripts/write-lp-file.py"


rule benchmark_memory_single_gurobi:
    input:
        lp="benchmarks/lp_files/{N}.lp"
    benchmark:
        "benchmarks/gurobi/solver/memory-{N}.txt"
    run:
        import gurobipy

        m = gurobipy.read(input.lp)
        m.optimize()


rule benchmark_time_single_gurobi:
    input:
        lp="benchmarks/lp_files/{N}.lp"
    output:
        time="benchmarks/gurobi/solver/time-{N}.txt"
    run:
        import gurobipy

        m = gurobipy.read(input.lp)
        m.optimize()

        with open(output.time, 'w') as f:
            f.write(str(m.RunTime))


rule benchmark_gurobi:
    params:
        nrange = NRANGE,
        api = "Solving Process",
    input:
        memory = expand("benchmarks/gurobi/solver/memory-{N}.txt", N=NRANGE),
        time = expand("benchmarks/gurobi/solver/time-{N}.txt", N=NRANGE),
    output:
        benchmark = "benchmarks/gurobi/solver.csv"
    script: "scripts/concat-benchmarks.py"


# Merge all benchmarks
rule merge_benchmarks:
    params:
        nrange = NRANGE
    input:
        benchmarks = lambda w: expand("benchmarks/{solver}/{api}.csv", solver=w.solver, api=APIS)
    output:
        absolute="benchmarks/{solver}/benchmarks-absolute.csv",
        overhead="benchmarks/{solver}/benchmarks-overhead.csv"
    script:
        "scripts/merge-benchmarks.py"



rule plot_benchmarks:
    input:
        "benchmarks/{solver}/benchmarks-{kind}.csv"
    output:
        "benchmarks/{solver}/benchmark-{kind}.pdf",
    notebook:
        "notebooks/plot-benchmarks.py.ipynb"
